{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinHern/SP1-Educational-Material/blob/main/examples/SP1_Example_ML_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2hOe4K0Hmut"
      },
      "source": [
        "## Preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEWUjG0IIJkk"
      },
      "source": [
        "# ----- Libraries ----- #\n",
        "\n",
        "# This is the main Library that allows us to work with Neural Networks\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# For graph plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.math import confusion_matrix\n",
        "\n",
        "# For dataset manipulation\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# For visualizing more complex graphs\n",
        "import seaborn as sns\n",
        "\n",
        "# Miscellaneous Libraries\n",
        "import os\n",
        "\n",
        "# Global constant for training acceleration\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARASHphaKU6D"
      },
      "source": [
        "# 1) Dataset Preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KMmvqGMGOXu"
      },
      "source": [
        "'''\n",
        "The dataset you are going to use is the following:\n",
        "https://archive.ics.uci.edu/ml/datasets/Wine+Quality\n",
        "'''\n",
        "\n",
        "raw_train_dataset = pd.read_csv(\"wine_quality_training.csv\")\n",
        "raw_train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Glimpse"
      ],
      "metadata": {
        "id": "OQ8ZBbZnuCz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Brief Statistical Summary of the dataset\n",
        "raw_train_dataset.describe()"
      ],
      "metadata": {
        "id": "eV1sMWWNuEg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check all columns' datatypes\n",
        "raw_train_dataset.dtypes"
      ],
      "metadata": {
        "id": "lWg0jab3uOHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking shape\n",
        "raw_train_dataset.shape"
      ],
      "metadata": {
        "id": "XWN-_MyBuP5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Pre-processing"
      ],
      "metadata": {
        "id": "XxAZgd0lxPQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing Values"
      ],
      "metadata": {
        "id": "eFUg8JU2wS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for Missing Values and do something about them!\n",
        "def missing_values_cleanup(dataset):\n",
        "  new_dataset = dataset.copy()\n",
        "\n",
        "  # Lets check for null values\n",
        "  print(new_dataset.isna().sum())\n",
        "\n",
        "  # TODO: Drop Missing values!\n",
        "\n",
        "  # Checking new dataset\n",
        "  return new_dataset"
      ],
      "metadata": {
        "id": "JBUeRdOwyCXX"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding"
      ],
      "metadata": {
        "id": "Q3XCVYydwUWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for All possible unique values for the String column in the train dataset\n",
        "# TODO: Check the values!"
      ],
      "metadata": {
        "id": "UsoBIqRszMP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for Test too!\n",
        "raw_test_dataset = pd.read_csv(\"wine_quality_evaluation.csv\")\n",
        "raw_test_dataset\n",
        "\n",
        "# TODO: Check test dataset values!"
      ],
      "metadata": {
        "id": "Qsz9rtmLc62V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With this information, we can perform mapping. We have to a define a function that maps all values\n",
        "# TODO: Finish this function!\n",
        "def map_fixed_acidity(x):\n",
        "  if x == 'Cero':\n",
        "    return None\n",
        "  elif x == 'One':\n",
        "    return None\n",
        "  elif x == 'Two':\n",
        "    return None\n",
        "  elif x == 'Three':\n",
        "    return None\n",
        "  elif x == 'Four':\n",
        "    return None\n",
        "  elif x == 'Five':\n",
        "    return None\n",
        "  elif x == 'Six':\n",
        "    return None\n",
        "  elif x == 'Seven':\n",
        "    return None\n",
        "  elif x == 'Eight':\n",
        "    return None\n",
        "  elif x == 'Nine':\n",
        "    return None\n",
        "  elif x == 'Ten':\n",
        "    return None\n",
        "  elif x == 'Eleven':\n",
        "    return None\n",
        "  elif x == 'Twelve':\n",
        "    return None\n",
        "  elif x == 'Thirteen':\n",
        "    return None\n",
        "  elif x == 'Fourteen':\n",
        "    return None\n",
        "  elif x == 'Fifteen':\n",
        "    return None\n",
        "  elif x == 'Seventeen':\n",
        "    return None\n",
        "  else:  # In case we missed a value, we return None\n",
        "    return None"
      ],
      "metadata": {
        "id": "c4GOBDH_zn8R"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoding_cleanup(dataset):\n",
        "  new_dataset = dataset.copy()\n",
        "\n",
        "  # TODO: Execute the mapping here!\n",
        "\n",
        "  return new_dataset"
      ],
      "metadata": {
        "id": "1n0rQJWR0Y_3"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization"
      ],
      "metadata": {
        "id": "4V_CT4ESwYAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization Functions"
      ],
      "metadata": {
        "id": "DDzFk3CP0smL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_min_max(column):\n",
        "  max_value = np.max(column)\n",
        "  min_value = np.min(column)\n",
        "  return (column - min_value)/(max_value - min_value)\n",
        "\n",
        "def z_normalization(column):\n",
        "    return (column - column.mean()) / column.std()"
      ],
      "metadata": {
        "id": "MEE8Ymwp0uN4"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_columns(dataset, columns):\n",
        "  new_dataset = dataset.copy()\n",
        "\n",
        "  # TODO: Execute the normalization\n",
        "  for column in columns:\n",
        "    pass\n",
        "\n",
        "  return new_dataset"
      ],
      "metadata": {
        "id": "dENAPwIJ1J8M"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets remember\n",
        "print(raw_train_dataset.columns)\n",
        "raw_train_dataset.dtypes"
      ],
      "metadata": {
        "id": "q-cRFeMGdstu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Fill the array\n",
        "columns_to_normalize = []\n",
        "columns_to_normalize"
      ],
      "metadata": {
        "id": "KgdL1voBMI0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balancing Classes"
      ],
      "metadata": {
        "id": "kdGi2dzAwZEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in raw_train_dataset[\"quality\"].unique():\n",
        "  numRows = len(raw_train_dataset[raw_train_dataset['quality'] == i])\n",
        "  print(\"Class\", i, \": \", numRows)\n",
        "\n",
        "# Setting style of the graph\n",
        "sns.set(style=\"darkgrid\")\n",
        "\n",
        "# Plotting a histogram\n",
        "sns.histplot(data=raw_train_dataset, x=\"quality\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UnFGKj1YxXPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping quality values\n",
        "# TODO: Complete the function!\n",
        "def map_quality(x):\n",
        "  pass"
      ],
      "metadata": {
        "id": "_ihQsHnO2MWm"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_dataset = raw_train_dataset.copy()\n",
        "\n",
        "dummy_dataset['quality'] = dummy_dataset['quality'].apply(map_quality)\n",
        "\n",
        "for i in dummy_dataset[\"quality\"].unique():\n",
        "  numRows = len(dummy_dataset[dummy_dataset['quality'] == i])\n",
        "  print(\"Class\", i, \": \", numRows)\n",
        "\n",
        "# Setting style of the graph\n",
        "sns.set(style=\"darkgrid\")\n",
        "\n",
        "# Plotting a histogram\n",
        "sns.histplot(data=dummy_dataset, x=\"quality\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tvTJM9OmePvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def undersample_classes(dataset, target_column, target_class):\n",
        "  new_dataset = dataset.copy()\n",
        "\n",
        "  # Lets extract how many classes there are in the target class\n",
        "  values = new_dataset[target_column].value_counts()\n",
        "  reference_class_count = values[target_class]\n",
        "\n",
        "  # Lets extract all data of the target class\n",
        "  undersampled_dataset = new_dataset[new_dataset[target_column] == target_class]\n",
        "\n",
        "  classes = list(new_dataset[target_column].unique())\n",
        "  classes.remove(target_class)\n",
        "\n",
        "  # TODO: Complete For loop\n",
        "  for data_class in classes:\n",
        "    pass\n",
        "\n",
        "  # Shuffling\n",
        "  undersampled_dataset = undersampled_dataset.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  return undersampled_dataset"
      ],
      "metadata": {
        "id": "Dp0Vdxyr3IHZ"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Executing Pre-Processing"
      ],
      "metadata": {
        "id": "Fwi4oUyrxbyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process_dataset(dataset, class_column, perform_undersample=False):\n",
        "  missingless_dataset = missing_values_cleanup(dataset=dataset)\n",
        "  encoded_dataset = encoding_cleanup(dataset=missingless_dataset)\n",
        "  normalized_dataset = normalize_columns(dataset=encoded_dataset, columns=columns_to_normalize)\n",
        "  normalized_dataset[class_column] = normalized_dataset[class_column].apply(map_quality)\n",
        "\n",
        "  pre_processed_dataset = None\n",
        "  if perform_undersample:\n",
        "    # TODO: Complete paramerets\n",
        "    pre_processed_dataset = undersample_classes(dataset=normalized_dataset, target_column=None, target_class=None)\n",
        "  else:\n",
        "    pre_processed_dataset = normalized_dataset\n",
        "\n",
        "  # Sepparating dependent and independent variables\n",
        "  independent_variables = list(pre_processed_dataset.columns)\n",
        "  independent_variables.remove(class_column)\n",
        "  dependent_variables = [class_column]\n",
        "\n",
        "  values_set = pre_processed_dataset[independent_variables]\n",
        "  values_target = pre_processed_dataset[dependent_variables]\n",
        "\n",
        "  return values_set, values_target"
      ],
      "metadata": {
        "id": "sUfR-pQgxflH"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) AI Model"
      ],
      "metadata": {
        "id": "qf_KThv9xowo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AI Model"
      ],
      "metadata": {
        "id": "5lyfFm59Vbfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(raw_train_dataset.columns)"
      ],
      "metadata": {
        "id": "2O3Hvs0fe-1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Build your own model!\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(0)),\n",
        "  tf.keras.layers.Dense(units=32, activation='relu'),\n",
        "  ])\n",
        "\n",
        "# TODO: Add the finishing touches!\n",
        "model.compile(loss=None, optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "XFWEAc2EVaAe"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callbacks"
      ],
      "metadata": {
        "id": "4cB7qBGKM9zN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Adjsut Earlystopping Callback\n",
        "earlystopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1, patience=1,)"
      ],
      "metadata": {
        "id": "bKYW1vDlM8gy"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up Tensorboard\n",
        "%load_ext tensorboard\n",
        "%mkdir logs & rm -rf ./logs/\n",
        "\n",
        "import datetime\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "1T1MvZLSVVGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architecture"
      ],
      "metadata": {
        "id": "xjwiPct6X1za"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "84Fpz-o6Xz3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Complete the Training\n",
        "train_set, train_target = (None, None)\n",
        "train_set"
      ],
      "metadata": {
        "id": "J6ZbUxTTfq25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set"
      ],
      "metadata": {
        "id": "xb4l949ciUVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(None,\n",
        "          None,\n",
        "          epochs=150,\n",
        "          batch_size=128,\n",
        "          validation_split=0.2,\n",
        "          callbacks=[earlystopping_callback, tensorboard_callback]\n",
        "        )"
      ],
      "metadata": {
        "id": "zYsnJfBnX630"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result Visualization"
      ],
      "metadata": {
        "id": "22-7F17iX44x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets open up the dashboard and check the training process\n",
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "56kqVHSIWP7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Evaluation"
      ],
      "metadata": {
        "id": "aDF8PtunxqbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Finish last step\n",
        "test_set, test_target = (None, None)\n",
        "\n",
        "model.evaluate(test_set, test_target)"
      ],
      "metadata": {
        "id": "hfhiTUjsWRxj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}